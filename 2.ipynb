{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score,classification_report,hamming_loss,f1_score,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login(key=\"a3ab9e880247c3575c105c3a5abb88ebc2eba87a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('WineQT.csv')\n",
    "X=data.loc[:,~data.columns.isin(['quality','Id'])].values\n",
    "y=data['quality'].values\n",
    "temp=np.zeros((y.shape[0],6), dtype=int)\n",
    "temp[np.arange(y.shape[0]), y - 3] = 1\n",
    "y_onehot=temp\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(X, y_onehot, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing and Normalising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar=StandardScaler()\n",
    "scalar.fit(x_train)\n",
    "x_train=scalar.transform(x_train)\n",
    "x_val=scalar.transform(x_val)\n",
    "x_test=scalar.transform(x_test)\n",
    "minmax_scalar=MinMaxScaler()\n",
    "minmax_scalar.fit(x_train)\n",
    "x_train=minmax_scalar.transform(x_train)\n",
    "x_val=minmax_scalar.transform(x_val)\n",
    "x_test=minmax_scalar.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "    def __init__(self,learning_rate,activation_function,optimizer,num_hidden_layers,count_neurons):\n",
    "        self.lrate=learning_rate\n",
    "        self.activation_func=activation_function\n",
    "        self.optimizer=optimizer\n",
    "        self.hidden_layers=num_hidden_layers\n",
    "        self.num_neurons=count_neurons\n",
    "        self.input_layer_size=None\n",
    "        self.ouput_layer_size=None\n",
    "        self.weights=None\n",
    "        self.biases=None\n",
    "        self.batch_size=100\n",
    "    def set_activation_function(self,af):\n",
    "        self.activation_func=af\n",
    "    def set_optimizer(self,opt):\n",
    "        self.optimizer=opt\n",
    "    def set_learning_rate(self,lr):\n",
    "        self.lrate=lr\n",
    "    def set_hidden_layers(self,hl):\n",
    "        self.hidden_layers=hl\n",
    "    def set_num_neurons(self,m):\n",
    "        self.num_neurons=m\n",
    "    def ReLu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "    def Tanh(self,z):\n",
    "        return (2/(1+np.exp(-2*z)))-1\n",
    "    def Sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    def Softmax(self,z):\n",
    "        expz=np.exp(z)\n",
    "        return expz/np.sum(expz,axis=1,keepdims=True)\n",
    "    def init_weights_biases(self):\n",
    "        weights=[]\n",
    "        biases=[]\n",
    "        for i in range(1,len(self.layer_sizes)):\n",
    "            weight_matrix = np.random.randn(self.layer_sizes[i-1],self.layer_sizes[i]) * math.sqrt(2.0/self.layer_sizes[i-1])\n",
    "            bias_vector = np.zeros((1,self.layer_sizes[i]))\n",
    "            weights.append(weight_matrix)\n",
    "            biases.append(bias_vector)\n",
    "        return weights, biases\n",
    "    def forward_propagation(self,X):\n",
    "        activations = [X]\n",
    "        z_values = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            z = activations[-1]@self.weights[i] + self.biases[i]\n",
    "            a = self.Sigmoid(z) if i < len(self.layer_sizes) - 2 else self.Softmax(z)\n",
    "            if self.activation_func=='tanh':\n",
    "                a = self.Tanh(z) if i < len(self.layer_sizes) - 2 else self.Softmax(z)\n",
    "            elif self.activation_func=='ReLu':\n",
    "                a = self.ReLu(z) if i < len(self.layer_sizes) - 2 else self.Softmax(z)\n",
    "            z_values.append(z)\n",
    "            activations.append(a) \n",
    "        return activations, z_values\n",
    "    def backward_propagation(self,y,activations):\n",
    "        grads = []\n",
    "        delta = activations[-1] - y\n",
    "        for i in range(len(self.layer_sizes) - 2, -1, -1):\n",
    "            dw = activations[i].T@delta\n",
    "            db = np.sum(delta, axis=0, keepdims=True)\n",
    "            grads.append((dw, db))\n",
    "            if i > 0:\n",
    "                if self.activation_func=='sigmoid':\n",
    "                    delta = (delta@self.weights[i].T) * activations[i] * (1 - activations[i])\n",
    "                elif self.activation_func=='tanh':\n",
    "                    delta=(delta@self.weights[i].T)*(1-np.square(activations[i]))\n",
    "                else:\n",
    "                    delta=(delta@self.weights[i].T)*((activations[i]>0).astype(int))\n",
    "        grads.reverse()\n",
    "        return grads\n",
    "    def compute_loss(self,y,y_pred):\n",
    "        return -np.sum(y * np.log(y_pred))\n",
    "    def update_parameters(self,grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lrate * grads[i][0]\n",
    "            self.biases[i] -= self.lrate * grads[i][1]\n",
    "    def train(self,x,y,x_val,y_val,num_epochs):\n",
    "        # nm=f\"LR:{self.lrate},HL:{self.hidden_layers},E:{num_epochs},HLN:{self.num_neurons}\"\n",
    "        # wandb.init(\n",
    "        #     project=\"MLPClassifier-EHLHLNLR\",\n",
    "        #     config={\n",
    "        #         \"Learning Rate\":self.lrate,\n",
    "        #         \"Epochs\":num_epochs,\n",
    "        #         \"Optimizer\":self.optimizer,\n",
    "        #         \"Layers\":self.hidden_layers,\n",
    "        #         \"LayerSize\":self.num_neurons\n",
    "        #     },\n",
    "        #     name=nm\n",
    "        # )\n",
    "        self.input_layer_size=x.shape[1]\n",
    "        self.ouput_layer_size=y.shape[1]\n",
    "        self.layer_sizes=[self.input_layer_size]+self.hidden_layers*[self.num_neurons]+[self.ouput_layer_size]\n",
    "        self.weights,self.biases=self.init_weights_biases()\n",
    "        m=x.shape[0]\n",
    "        if self.optimizer=='mbgd':\n",
    "            for _ in range(num_epochs):\n",
    "                permutation = np.random.permutation(m)\n",
    "                x_shuffled = x[permutation,:]\n",
    "                y_shuffled = y[permutation,:]\n",
    "                for i in range(0, m, self.batch_size):\n",
    "                    x_batch = x_shuffled[i:i + self.batch_size,:]\n",
    "                    y_batch = y_shuffled[i:i + self.batch_size,:]\n",
    "                    if len(x_batch) < self.batch_size:\n",
    "                        remaining_samples =m % self.batch_size\n",
    "                        x_batch = x_shuffled[i:i+remaining_samples]\n",
    "                        y_batch = y_shuffled[i:i+remaining_samples]\n",
    "                    activations,_ = self.forward_propagation(x_batch)\n",
    "                    loss = self.compute_loss(y_batch, activations[-1])\n",
    "                    self.final_loss=loss\n",
    "                    grads = self.backward_propagation(y_batch, activations)\n",
    "                    self.update_parameters(grads)\n",
    "                # activations, _ = self.forward_propagation(x_val)\n",
    "                # pred_val=activations[-1]\n",
    "                # loss_val=self.compute_loss(y_val,pred_val)\n",
    "                # max_indices = np.argmax(pred_val, axis=1).reshape(-1, 1)\n",
    "                # new_matrix = np.zeros((max_indices.shape[0], 6), dtype=int)\n",
    "                # rows, cols = np.arange(max_indices.shape[0]), max_indices.flatten()\n",
    "                # new_matrix[rows, cols] = 1\n",
    "                # accuracy_val=accuracy_score(y_val,new_matrix)\n",
    "                # activations, _ = self.forward_propagation(x)\n",
    "                # pred_train=activations[-1]\n",
    "                # loss_train=self.compute_loss(y,pred_train)\n",
    "                # max_indices = np.argmax(pred_train, axis=1).reshape(-1, 1)\n",
    "                # new_matrix = np.zeros((max_indices.shape[0], 6), dtype=int)\n",
    "                # rows, cols = np.arange(max_indices.shape[0]), max_indices.flatten()\n",
    "                # new_matrix[rows, cols] = 1\n",
    "                # accuracy_train=accuracy_score(y,new_matrix)\n",
    "                # wandb.log({'val_loss':loss_val,'train_loss':loss_train,'train_accuracy':accuracy_train,'val_accuracy':accuracy_val})\n",
    "        elif self.optimizer=='gd':\n",
    "            for _ in range(num_epochs):\n",
    "                activations,_ = self.forward_propagation(x)\n",
    "                loss = self.compute_loss(y, activations[-1])\n",
    "                self.final_loss=loss\n",
    "                grads = self.backward_propagation(y, activations)\n",
    "                self.update_parameters(grads)\n",
    "                # activations, _ = self.forward_propagation(x_val)\n",
    "                # pred_val=activations[-1]\n",
    "                # loss_val=self.compute_loss(y_val,pred_val)\n",
    "                # max_indices = np.argmax(pred_val, axis=1).reshape(-1, 1)\n",
    "                # new_matrix = np.zeros((max_indices.shape[0], 6), dtype=int)\n",
    "                # rows, cols = np.arange(max_indices.shape[0]), max_indices.flatten()\n",
    "                # new_matrix[rows, cols] = 1\n",
    "                # accuracy_val=accuracy_score(y_val,new_matrix)\n",
    "                # activations, _ = self.forward_propagation(x)\n",
    "                # pred_train=activations[-1]\n",
    "                # loss_train=self.compute_loss(y,pred_train)\n",
    "                # max_indices = np.argmax(pred_train, axis=1).reshape(-1, 1)\n",
    "                # new_matrix = np.zeros((max_indices.shape[0], 6), dtype=int)\n",
    "                # rows, cols = np.arange(max_indices.shape[0]), max_indices.flatten()\n",
    "                # new_matrix[rows, cols] = 1\n",
    "                # accuracy_train=accuracy_score(y,new_matrix)\n",
    "                # wandb.log({'val_loss':loss_val,'train_loss':loss_train,'train_accuracy':accuracy_train,'val_accuracy':accuracy_val})\n",
    "        else:\n",
    "            for _ in range(num_epochs):\n",
    "                permutation = np.random.permutation(m)\n",
    "                x_shuffled = x[permutation,:]\n",
    "                y_shuffled = y[permutation,:]\n",
    "                for i in range(m):\n",
    "                    x_sample = x_shuffled[i:i+1,:]\n",
    "                    y_sample = y_shuffled[i:i+1,:]\n",
    "                    activations,_ = self.forward_propagation(x_sample)\n",
    "                    loss = self.compute_loss(y_sample, activations[-1])\n",
    "                    self.final_loss=loss\n",
    "                    grads = self.backward_propagation(y_sample, activations)\n",
    "                    self.update_parameters(grads)\n",
    "                # activations, _ = self.forward_propagation(x_val)\n",
    "                # pred_val=activations[-1]\n",
    "                # loss_val=self.compute_loss(y_val,pred_val)\n",
    "                # max_indices = np.argmax(pred_val, axis=1).reshape(-1, 1)\n",
    "                # new_matrix = np.zeros((max_indices.shape[0], 6), dtype=int)\n",
    "                # rows, cols = np.arange(max_indices.shape[0]), max_indices.flatten()\n",
    "                # new_matrix[rows, cols] = 1\n",
    "                # accuracy_val=accuracy_score(y_val,new_matrix)\n",
    "                # activations, _ = self.forward_propagation(x)\n",
    "                # pred_train=activations[-1]\n",
    "                # loss_train=self.compute_loss(y,pred_train)\n",
    "                # max_indices = np.argmax(pred_train, axis=1).reshape(-1, 1)\n",
    "                # new_matrix = np.zeros((max_indices.shape[0], 6), dtype=int)\n",
    "                # rows, cols = np.arange(max_indices.shape[0]), max_indices.flatten()\n",
    "                # new_matrix[rows, cols] = 1\n",
    "                # accuracy_train=accuracy_score(y,new_matrix)\n",
    "                # wandb.log({'val_loss':loss_val,'train_loss':loss_train,'train_accuracy':accuracy_train,'val_accuracy':accuracy_val})\n",
    "    def predict(self,x):\n",
    "        activations, _ = self.forward_propagation(x)\n",
    "        return activations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score on test set : 0.5988372093023255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      0.00      0.00         6\n",
      "           5       0.69      0.73      0.71        81\n",
      "           6       0.53      0.63      0.57        65\n",
      "           7       0.38      0.18      0.24        17\n",
      "           8       1.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.60       172\n",
      "   macro avg       0.72      0.31      0.30       172\n",
      "weighted avg       0.61      0.60      0.57       172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cl=MLPClassifier(0.0001,'sigmoid','gd',2,7)\n",
    "cl.train(x_train,y_train,x_val,y_val,10000)\n",
    "l=cl.predict(x_test)\n",
    "max_indices = np.argmax(l, axis=1).reshape(-1, 1)\n",
    "new_matrix = np.zeros((max_indices.shape[0], 6), dtype=int)\n",
    "rows, cols = np.arange(max_indices.shape[0]), max_indices.flatten()\n",
    "new_matrix[rows, cols] = 1\n",
    "print(f'Accuracy Score on test set : {accuracy_score(y_test,new_matrix)}')\n",
    "y_trueaccrep=np.argmax(y_test, axis=1).reshape(-1, 1)+3\n",
    "print(classification_report(y_trueaccrep,max_indices+3,zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrates=[0.001,0.0001]\n",
    "# epochs=[5000,10000]\n",
    "# hln=[4,7]\n",
    "# hl=[2,4]\n",
    "# for lra in lrates:\n",
    "#     for ep in epochs:\n",
    "#         for a in hln:\n",
    "#             for b in hl:\n",
    "#                 cl=MLPClassifier(lra,'sigmoid','gd',b,a)\n",
    "#                 cl.train(x_train,y_train,x_val,y_val,ep)\n",
    "#                 l=cl.predict(x_test)\n",
    "#                 max_indices = np.argmax(l, axis=1).reshape(-1, 1)\n",
    "#                 new_matrix = np.zeros((max_indices.shape[0], 6), dtype=int)\n",
    "#                 rows, cols = np.arange(max_indices.shape[0]), max_indices.flatten()\n",
    "#                 new_matrix[rows, cols] = 1\n",
    "#                 wandb.log({'test_accuracy':100*accuracy_score(y_test,new_matrix),'test_f1score':100*f1_score(y_test,new_matrix,average=\"macro\"),'test_recall':100*recall_score(y_test,new_matrix,average=\"macro\"),'test_precision':100*precision_score(y_test,new_matrix,average=\"macro\")})\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afs=['sigmoid','ReLu','tanh']\n",
    "# opt=['gd','sgd','mbgd']\n",
    "# for o in opt:\n",
    "#     for af in afs:\n",
    "#         cl=MLPClassifier(0.001,af,o,2,10)\n",
    "#         cl.train(x_train,y_train,x_val,y_val,5000)\n",
    "#         l=cl.predict(x_test)\n",
    "#         max_indices = np.argmax(l, axis=1).reshape(-1, 1)\n",
    "#         new_matrix = np.zeros((max_indices.shape[0], 6), dtype=int)\n",
    "#         rows, cols = np.arange(max_indices.shape[0]), max_indices.flatten()\n",
    "#         new_matrix[rows, cols] = 1\n",
    "#         wandb.log({'test_accuracy':100*accuracy_score(y_test,new_matrix),'test_f1score':100*f1_score(y_test,new_matrix,average=\"macro\"),'test_recall':100*recall_score(y_test,new_matrix,average=\"macro\"),'test_precision':100*precision_score(y_test,new_matrix,average=\"macro\")})\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The MLP classifier gives a slightly better accuracy than the multinomial logistic regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('advertisement.csv')\n",
    "data=data.drop('city', axis=1)\n",
    "x=data.iloc[:, :-1]\n",
    "y=data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wtoi(y):\n",
    "    vis=\"\"\n",
    "    for s in y:\n",
    "        vis+=s\n",
    "        vis+=\" \"\n",
    "    words = vis.split()\n",
    "    word_counts = Counter(words) \n",
    "    word_list = list(word_counts.keys())\n",
    "    uniq_labels=sorted(word_list)\n",
    "    return {word: index for index, word in enumerate(uniq_labels)}\n",
    "\n",
    "def encode_labels(y):\n",
    "        word_to_index=wtoi(y)\n",
    "        vis=\"\"\n",
    "        for s in y:\n",
    "            vis+=s\n",
    "            vis+=\" \"\n",
    "        words = vis.split()\n",
    "        word_counts = Counter(words) \n",
    "        word_list = list(word_counts.keys())\n",
    "        uniq_labels=sorted(word_list)\n",
    "        uniq_count=len(uniq_labels)\n",
    "        word_to_index = {word: index for index, word in enumerate(uniq_labels)}\n",
    "        vecs=[]\n",
    "        for spl in y:\n",
    "            new_label=[0]*uniq_count\n",
    "            present=spl.split()\n",
    "            for w in present:\n",
    "                new_label[word_to_index[w]]=1\n",
    "            vecs.append(new_label)\n",
    "        return np.array(vecs)\n",
    "\n",
    "y_onehot=encode_labels(y)\n",
    "\n",
    "def onehot(X):\n",
    "    cols=['gender','education','married','occupation','most bought item']\n",
    "    x_new=pd.get_dummies(X,columns=cols)\n",
    "    return x_new.values\n",
    "\n",
    "x_onehot=onehot(x)\n",
    "x_onehot[:,4:]=x_onehot[:,4:].astype(float)\n",
    "mms=MinMaxScaler()\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_onehot,y_onehot,test_size=0.3,random_state=42)\n",
    "x_test,x_val,y_test,y_val=train_test_split(x_test,y_test,test_size=0.5,random_state=42)\n",
    "x_train=mms.fit_transform(x_train)\n",
    "x_test=mms.transform(x_test)\n",
    "x_val=mms.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMultiLabelClassifier:\n",
    "    def __init__(self,learning_rate,activation_function,optimizer,num_hidden_layers,count_neurons):\n",
    "        self.lrate=learning_rate\n",
    "        self.activation_func=activation_function\n",
    "        self.optimizer=optimizer\n",
    "        self.hidden_layers=num_hidden_layers\n",
    "        self.num_neurons=count_neurons\n",
    "        self.input_layer_size=None\n",
    "        self.ouput_layer_size=None\n",
    "        self.weights=None\n",
    "        self.biases=None\n",
    "        self.batch_size=100\n",
    "    def set_activation_function(self,af):\n",
    "        self.activation_func=af\n",
    "    def set_optimizer(self,opt):\n",
    "        self.optimizer=opt\n",
    "    def set_learning_rate(self,lr):\n",
    "        self.lrate=lr\n",
    "    def set_hidden_layers(self,hl):\n",
    "        self.hidden_layers=hl\n",
    "    def set_num_neurons(self,m):\n",
    "        self.num_neurons=m\n",
    "    def ReLu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "    def Tanh(self,z):\n",
    "        return (2/(1+np.exp(-2*z)))-1\n",
    "    def Sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    def init_weights_biases(self):\n",
    "        weights=[]\n",
    "        biases=[]\n",
    "        for i in range(1,len(self.layer_sizes)):\n",
    "            weight_matrix = np.random.randn(self.layer_sizes[i-1],self.layer_sizes[i]) * math.sqrt(2.0/self.layer_sizes[i-1])\n",
    "            bias_vector = np.zeros((1,self.layer_sizes[i]))\n",
    "            weights.append(weight_matrix)\n",
    "            biases.append(bias_vector)\n",
    "        return weights, biases\n",
    "    def forward_propagation(self,X):\n",
    "        activations = [X]\n",
    "        z_values = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            z = activations[-1]@self.weights[i] + self.biases[i]\n",
    "            a = self.Sigmoid(z)\n",
    "            if self.activation_func=='tanh':\n",
    "                a = self.Tanh(z) if i < len(self.layer_sizes) - 2 else self.Sigmoid(z)\n",
    "            elif self.activation_func=='ReLu':\n",
    "                a = self.ReLu(z) if i < len(self.layer_sizes) - 2 else self.Sigmoid(z)\n",
    "            z_values.append(z)\n",
    "            activations.append(a) \n",
    "        return activations, z_values\n",
    "    def backward_propagation(self,y,activations):\n",
    "        grads = []\n",
    "        delta = activations[-1] - y\n",
    "        for i in range(len(self.layer_sizes) - 2, -1, -1):\n",
    "            dw = activations[i].T@delta\n",
    "            db = np.sum(delta, axis=0, keepdims=True)\n",
    "            grads.append((dw, db))\n",
    "            if i > 0:\n",
    "                if self.activation_func=='sigmoid':\n",
    "                    delta = (delta@self.weights[i].T) * activations[i] * (1 - activations[i])\n",
    "                elif self.activation_func=='tanh':\n",
    "                    delta=(delta@self.weights[i].T)*(1-np.square(activations[i]))\n",
    "                else:\n",
    "                    delta=(delta@self.weights[i].T)*((activations[i]>0).astype(int))\n",
    "        grads.reverse()\n",
    "        return grads\n",
    "    def compute_loss(self,y,y_pred):\n",
    "        return -np.sum(y * np.log(y_pred))\n",
    "    def update_parameters(self,grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lrate * grads[i][0]\n",
    "            self.biases[i] -= self.lrate * grads[i][1]\n",
    "    def train(self,x,y,x_val,y_val,num_epochs):\n",
    "        # nm=f\"LR:{self.lrate},HL:{self.hidden_layers},HLN:{self.num_neurons},E:{num_epochs}\"\n",
    "        # wandb.init(\n",
    "        #     project=\"MultiLabel-MLP-EHLHLNLR\",\n",
    "        #     config={\n",
    "        #         \"Learning Rate\":self.lrate,\n",
    "        #         \"Epochs\":num_epochs,\n",
    "        #         \"Optimizer\":self.optimizer,\n",
    "        #         \"Layers\":self.hidden_layers,\n",
    "        #         \"LayerSize\":self.num_neurons\n",
    "        #     },\n",
    "        #     name=nm\n",
    "        # )\n",
    "        self.input_layer_size=x.shape[1]\n",
    "        self.ouput_layer_size=y.shape[1]\n",
    "        self.layer_sizes=[self.input_layer_size]+self.hidden_layers*[self.num_neurons]+[self.ouput_layer_size]\n",
    "        self.weights,self.biases=self.init_weights_biases()\n",
    "        m=x.shape[0]\n",
    "        if self.optimizer=='mbgd':\n",
    "            for _ in range(num_epochs):\n",
    "                permutation = np.random.permutation(m)\n",
    "                x_shuffled = x[permutation,:]\n",
    "                y_shuffled = y[permutation,:]\n",
    "                for i in range(0, m, self.batch_size):\n",
    "                    x_batch = x_shuffled[i:i + self.batch_size,:]\n",
    "                    y_batch = y_shuffled[i:i + self.batch_size,:]\n",
    "                    if len(x_batch) < self.batch_size:\n",
    "                        remaining_samples =m % self.batch_size\n",
    "                        x_batch = x_shuffled[i:i+remaining_samples]\n",
    "                        y_batch = y_shuffled[i:i+remaining_samples]\n",
    "                    activations,_ = self.forward_propagation(x_batch)\n",
    "                    loss = self.compute_loss(y_batch, activations[-1])\n",
    "                    self.final_loss=loss\n",
    "                    grads = self.backward_propagation(y_batch, activations)\n",
    "                    self.update_parameters(grads)\n",
    "                # activations, _ = self.forward_propagation(x_val)\n",
    "                # pred_val=activations[-1]\n",
    "                # loss_val=self.compute_loss(y_val,pred_val)\n",
    "                # pred_val = (pred_val > 0.5).astype(int)\n",
    "                # accuracy_val=accuracy_score(y_val,pred_val)\n",
    "                # activations, _ = self.forward_propagation(x)\n",
    "                # pred_train=activations[-1]\n",
    "                # loss_train=self.compute_loss(y,pred_train)\n",
    "                # pred_train=(pred_train > 0.5).astype(int)\n",
    "                # accuracy_train=accuracy_score(y,pred_train)\n",
    "                # wandb.log({'val_loss':loss_val,'train_loss':loss_train,'train_accuracy':accuracy_train,'val_accuracy':accuracy_val})\n",
    "        elif self.optimizer=='gd':\n",
    "            for _ in range(num_epochs):\n",
    "                activations,_ = self.forward_propagation(x)\n",
    "                loss = self.compute_loss(y, activations[-1])\n",
    "                self.final_loss=loss\n",
    "                grads = self.backward_propagation(y, activations)\n",
    "                self.update_parameters(grads)\n",
    "                # logging\n",
    "                # activations, _ = self.forward_propagation(x_val)\n",
    "                # pred_val=activations[-1]\n",
    "                # loss_val=self.compute_loss(y_val,pred_val)\n",
    "                # pred_val = (pred_val > 0.5).astype(int)\n",
    "                # accuracy_val=accuracy_score(y_val,pred_val)\n",
    "                # activations, _ = self.forward_propagation(x)\n",
    "                # pred_train=activations[-1]\n",
    "                # loss_train=self.compute_loss(y,pred_train)\n",
    "                # pred_train=(pred_train > 0.5).astype(int)\n",
    "                # accuracy_train=accuracy_score(y,pred_train)\n",
    "                # wandb.log({'val_loss':loss_val,'train_loss':loss_train,'train_accuracy':accuracy_train,'val_accuracy':accuracy_val})\n",
    "        else:\n",
    "            for _ in range(num_epochs):\n",
    "                permutation = np.random.permutation(m)\n",
    "                x_shuffled = x[permutation,:]\n",
    "                y_shuffled = y[permutation,:]\n",
    "                for i in range(m):\n",
    "                    x_sample = x_shuffled[i:i+1,:]\n",
    "                    y_sample = y_shuffled[i:i+1,:]\n",
    "                    activations,_ = self.forward_propagation(x_sample)\n",
    "                    loss = self.compute_loss(y_sample, activations[-1])\n",
    "                    self.final_loss=loss\n",
    "                    grads = self.backward_propagation(y_sample, activations)\n",
    "                    self.update_parameters(grads)\n",
    "                #logging\n",
    "                # activations, _ = self.forward_propagation(x_val)\n",
    "                # pred_val=activations[-1]\n",
    "                # loss_val=self.compute_loss(y_val,pred_val)\n",
    "                # pred_val = (pred_val > 0.5).astype(int)\n",
    "                # accuracy_val=accuracy_score(y_val,pred_val)\n",
    "                # activations, _ = self.forward_propagation(x)\n",
    "                # pred_train=activations[-1]\n",
    "                # loss_train=self.compute_loss(y,pred_train)\n",
    "                # pred_train=(pred_train > 0.5).astype(int)\n",
    "                # accuracy_train=accuracy_score(y,pred_train)\n",
    "                # wandb.log({'val_loss':loss_val,'train_loss':loss_train,'train_accuracy':accuracy_train,'val_accuracy':accuracy_val})\n",
    "    def predict(self,x):\n",
    "        activations, _ = self.forward_propagation(x)\n",
    "        return activations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for test set : 5.333333333333334%\n",
      "F1-Score for test set : 0.5259849718034559\n",
      "Recall Score for test set : 0.5126408882439127\n",
      "Precision Score for test set : 0.5539904831365337\n"
     ]
    }
   ],
   "source": [
    "cl=MLPMultiLabelClassifier(0.001,'tanh','mbgd',3,8)\n",
    "cl.train(x_train,y_train,x_val,y_val,5000)\n",
    "l=cl.predict(x_test)\n",
    "pred = (l > 0.5).astype(int)\n",
    "print(f'Accuracy Score for test set : {100*accuracy_score(y_test,pred)}%')\n",
    "print(f'F1-Score for test set : {f1_score(y_test,pred,average=\"macro\")}')\n",
    "print(f'Recall Score for test set : {recall_score(y_test,pred,average=\"macro\")}')\n",
    "print(f'Precision Score for test set : {precision_score(y_test,pred,average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrates=[0.0001,0.00001]\n",
    "# epochs=[5000,10000]\n",
    "# hln=[8,10]\n",
    "# hl=[2,4]\n",
    "# for lra in lrates:\n",
    "#     for ep in epochs:\n",
    "#         for a in hln:\n",
    "#             for b in hl:\n",
    "#                 cl=MLPMultiLabelClassifier(lra,'sigmoid','gd',b,a)\n",
    "#                 cl.train(x_train,y_train,x_val,y_val,ep)\n",
    "#                 l=cl.predict(x_test)\n",
    "#                 pred = (l > 0.5).astype(int)\n",
    "#                 wandb.log({'test_accuracy':100*accuracy_score(y_test,pred),'test_f1score':100*f1_score(y_test,pred,average=\"macro\"),'test_recall':100*recall_score(y_test,pred,average=\"macro\"),'test_precision':100*precision_score(y_test,pred,average=\"macro\")})\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afs=['sigmoid','ReLu','tanh']\n",
    "# opt=['gd','sgd','mbgd']\n",
    "# for o in opt:\n",
    "#     for af in afs:\n",
    "#         cl=MLPMultiLabelClassifier(0.001,af,o,2,10)\n",
    "#         cl.train(x_train,y_train,x_val,y_val,5000)\n",
    "#         l=cl.predict(x_test)\n",
    "#         pred = (l > 0.5).astype(int)\n",
    "#         wandb.log({'test_accuracy':100*accuracy_score(y_test,pred),'test_f1score':100*f1_score(y_test,pred,average=\"macro\"),'test_recall':100*recall_score(y_test,pred,average=\"macro\"),'test_precision':100*precision_score(y_test,pred,average=\"macro\")})\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
